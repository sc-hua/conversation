#!/usr/bin/env python3
"""
Simple test script for Ollama qwen2.5vl:3b integration.

Tests the complete conversation system with Ollama.
"""

import asyncio
import os
from dotenv import load_dotenv
from conversation_graph import ConversationGraph
from modules import StructuredMessageContent

# Load environment variables
load_dotenv()


async def test_ollama_conversation():
    """Test a simple conversation with Ollama qwen2.5vl:3b."""
    print("ğŸ¤– Testing Ollama qwen2.5vl:3b Conversation")
    print("=" * 50)
    
    # Create conversation graph
    graph = ConversationGraph()
    
    # Test 1: Simple text conversation
    print("\n1ï¸âƒ£ Simple text conversation:")
    simple_content = StructuredMessageContent()
    simple_content.add_text("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚")
    
    result1 = await graph.chat(
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œä½¿ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚",
        structured_content=simple_content
    )
    
    print(f"è¾“å…¥: {result1['input_preview']}")
    print(f"å›å¤: {result1['response']}")
    
    # Test 2: Structured content with JSON
    print("\n2ï¸âƒ£ Structured content with JSON:")
    structured_content = StructuredMessageContent()
    structured_content.add_text("è¯·åˆ†æä»¥ä¸‹æ•°æ®ï¼š")
    structured_content.add_json({
        "é”€å”®é¢": 150000,
        "å®¢æˆ·æ•°": 245,
        "å¢é•¿ç‡": "15%",
        "åœ°åŒº": "åä¸œ"
    })
    structured_content.add_text("è¿™äº›é”€å”®æ•°æ®è¡¨ç°å¦‚ä½•ï¼Ÿ")
    
    result2 = await graph.chat(
        conversation_id=result1['conversation_id'],  # Continue same conversation
        structured_content=structured_content
    )
    
    print(f"è¾“å…¥: {result2['input_preview']}")
    print(f"å›å¤: {result2['response']}")
    
    # Test 3: Multi-modal with image reference
    print("\n3ï¸âƒ£ Multi-modal content:")
    multimodal_content = StructuredMessageContent()
    multimodal_content.add_text("æˆ‘æœ‰ä¸€å¼ å›¾ç‰‡å’Œæ•°æ®éœ€è¦åˆ†æ")
    multimodal_content.add_image("data_chart.png")
    multimodal_content.add_json({
        "å›¾ç‰‡ç±»å‹": "æ•°æ®å›¾è¡¨",
        "æ•°æ®ç‚¹": 12,
        "è¶‹åŠ¿": "ä¸Šå‡"
    })
    multimodal_content.add_text("è¯·ç»¼åˆåˆ†æå›¾ç‰‡å’Œæ•°æ®")
    
    result3 = await graph.chat(
        conversation_id=result1['conversation_id'],
        structured_content=multimodal_content,
        is_final=True  # Save conversation to file
    )
    
    print(f"è¾“å…¥: {result3['input_preview']}")
    print(f"å›å¤: {result3['response']}")
    
    print(f"\nâœ… å¯¹è¯å®Œæˆ! å¯¹è¯ID: {result1['conversation_id']}")
    print(f"ğŸ“Š æ€»æ¶ˆæ¯æ•°: {result3['message_count']}")


async def main():
    """Run the test."""
    current_llm = os.getenv('LLM_TYPE', 'mock')
    print(f"å½“å‰LLMé…ç½®: {current_llm.upper()}")
    
    if current_llm == 'ollama':
        model = os.getenv('OLLAMA_MODEL', 'qwen2.5vl:3b')
        url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        print(f"Ollamaæ¨¡å‹: {model}")
        print(f"Ollamaåœ°å€: {url}")
    
    try:
        await test_ollama_conversation()
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    asyncio.run(main())
