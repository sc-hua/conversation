#!/usr/bin/env python3
"""
LLM ç±»å‹åˆ‡æ¢å™¨ - ç”¨äºåœ¨ä¸åŒ LLM ç±»å‹ä¹‹é—´æ–¹ä¾¿åˆ‡æ¢çš„è„šæœ¬ã€‚

ç”¨æ³•:
    python switch_llm.py mock
    python switch_llm.py ollama
    python switch_llm.py openai
"""

import sys
import os
from pathlib import Path


def switch_llm_type(new_type):
    """åœ¨ .env æ–‡ä»¶ä¸­åˆ‡æ¢ LLM ç±»å‹ã€‚"""
    env_file = Path('.env')
    
    if not env_file.exists():
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° .env æ–‡ä»¶")
        return False
    
    # Valid LLM types
    valid_types = ['mock', 'ollama', 'openai']
    if new_type not in valid_types:
        print(f"âŒ é”™è¯¯ï¼šæ— æ•ˆçš„ LLM ç±»å‹ '{new_type}'")
        print(f"å¯é€‰ç±»å‹ï¼š{', '.join(valid_types)}")
        return False
    
    # Read current .env file
    lines = []
    with open(env_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # Update LLM_TYPE line
    updated = False
    for i, line in enumerate(lines):
        if line.startswith('LLM_TYPE='):
            lines[i] = f'LLM_TYPE={new_type}\n'
            updated = True
            break
    
    if not updated:
        # Add LLM_TYPE if not found
        lines.insert(1, f'LLM_TYPE={new_type}\n')
    
    # Write back to .env file
    with open(env_file, 'w', encoding='utf-8') as f:
        f.writelines(lines)
    
    print(f"âœ… LLM ç±»å‹å·²åˆ‡æ¢ä¸º: {new_type.upper()}")
    
    # Show current configuration
    if new_type == 'ollama':
        model = os.getenv('OLLAMA_MODEL', 'qwen2.5vl:3b')
        url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        print(f"   Model: {model}")
        print(f"   URL: {url}")
    elif new_type == 'openai':
        model = os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo')
        url = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
        print(f"   Model: {model}")
        print(f"   URL: {url}")
        api_key = os.getenv('OPENAI_API_KEY', 'your_openai_api_key_here')
        if api_key == 'your_openai_api_key_here':
            print("   âš ï¸  Warning: Please set OPENAI_API_KEY in .env file")
    
    return True


def show_usage():
    """æ˜¾ç¤ºä½¿ç”¨è¯´æ˜ã€‚"""
    print("LLM ç±»å‹åˆ‡æ¢å™¨")
    print("=" * 30)
    print("ç”¨æ³•: python switch_llm.py <type>")
    print()
    print("Available types:")
    print("  mock   - ç”¨äºæµ‹è¯•çš„ Mock LLMï¼ˆæ— éœ€å¤–éƒ¨ä¾èµ–ï¼‰")
    print("  ollama - Ollama qwen2.5vl:3bï¼ˆéœ€è¿è¡Œ Ollamaï¼‰")
    print("  openai - OpenAI GPT ç³»åˆ—æ¨¡å‹ï¼ˆéœ€é…ç½® API Keyï¼‰")
    print()
    print("Examples:")
    print("  python switch_llm.py mock")
    print("  python switch_llm.py ollama")
    print("  python switch_llm.py openai")


def main():
    """ä¸»å‡½æ•°ã€‚"""
    if len(sys.argv) != 2:
        show_usage()
        return
    
    new_type = sys.argv[1].lower()
    
    if new_type in ['-h', '--help', 'help']:
        show_usage()
        return
    
    success = switch_llm_type(new_type)
    
    if success:
        print()
        print("ğŸš€ ç°åœ¨å¯ä»¥æµ‹è¯•å¯¹è¯ç³»ç»Ÿäº†ï¼š")
        print("   python test_llm.py")
        print("   python test_ollama_chat.py")
        print("   python examples.py")


if __name__ == "__main__":
    main()
