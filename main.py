"""ç‹¬ç«‹ç‰ˆå¯¹è¯ç³»ç»Ÿç¤ºä¾‹ï¼Œä½¿ç”¨ç°æœ‰æ¨¡å—é¿å…é‡å¤å®šä¹‰ã€‚"""

import asyncio
import uuid
from typing import Dict, List, Optional, Any
from dotenv import load_dotenv

# ä½¿ç”¨ç°æœ‰æ¨¡å—ï¼Œé¿å…é‡å¤å®šä¹‰
from models import Message, StructuredMessageContent
from conversation_manager import ConversationManager

load_dotenv()


class MockLLM:
    """æ¨¡æ‹Ÿè¯­è¨€æ¨¡å‹ï¼Œç”¨äºæ—  API ä¾èµ–çš„æµ‹è¯•ã€‚"""
    
    async def generate_response(self, messages: List[Message], 
                                current_input: StructuredMessageContent) -> str:
        """åŸºäºç»“æ„åŒ–è¾“å…¥ç”Ÿæˆæ¨¡æ‹Ÿå›å¤æ–‡æœ¬ã€‚"""
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ

        responses = ["å·²åˆ†ææ‚¨çš„å†…å®¹ï¼š"]

        # å¤„ç†æ¯ä¸ªå†…å®¹å—
        for i, block in enumerate(current_input.blocks):
            block_desc = f"ç¬¬{i+1}é¡¹"

            if block.type == "text":
                responses.append(f"{block_desc}: æ–‡æœ¬ - {block.content}")
            elif block.type == "image":
                responses.append(f"{block_desc}: å›¾ç‰‡ - {block.content}")
            elif block.type == "json":
                key_count = len(block.content) if isinstance(block.content, dict) else 0
                responses.append(f"{block_desc}: JSON åŒ…å« {key_count} ä¸ªå­—æ®µ")

        # æ·»åŠ å¯¹è¯ä¸Šä¸‹æ–‡ä¿¡æ¯
        user_messages = [msg for msg in messages if msg.role == "user"]
        if len(user_messages) > 0:
            responses.append(f"è¿™æ˜¯ç¬¬ {len(user_messages) + 1} æ¬¡äº¤äº’ã€‚")

        return " ".join(responses)


class StandaloneConversationGraph:
    """ç‹¬ç«‹å¯¹è¯å›¾ï¼šå¤„ç†è¾“å…¥ã€ç”Ÿæˆå›å¤å¹¶ä¿å­˜å†å²ã€‚"""
    
    def __init__(self, max_concurrent: int = 5):
        """åˆå§‹åŒ–ï¼šmax_concurrent æ§åˆ¶å¹¶å‘é‡ã€‚"""
        self.llm = MockLLM()
        self.conversation_manager = ConversationManager()  # ä½¿ç”¨ç°æœ‰çš„ç®¡ç†å™¨
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def chat(self,
                   conversation_id: Optional[str] = None,
                   system_prompt: Optional[str] = None,
                   structured_content: Optional[StructuredMessageContent] = None,
                   is_final: bool = False
                   ) -> Dict[str, Any]:
        """ä¸»èŠå¤©æ¥å£ï¼šå¤„ç†è¾“å…¥ã€è°ƒç”¨ LLMã€ä¿å­˜å†å²å¹¶è¿”å›ç»“æœå­—å…¸ã€‚"""
        async with self.semaphore:  # æ§åˆ¶å¹¶å‘
            # ç”Ÿæˆå¯¹è¯ ID
            conv_id = conversation_id or str(uuid.uuid4())
            
            # è·å–ç°æœ‰å¯¹è¯å†å²
            messages = self.conversation_manager.get_conversation_history(conv_id)
            
            # å¦‚æœæ˜¯ç¬¬ä¸€æ¡æ¶ˆæ¯åˆ™æ·»åŠ ç³»ç»Ÿæç¤º
            if not messages and system_prompt:
                system_msg = Message(role="system", content=system_prompt)
                messages.append(system_msg)
                self.conversation_manager.save_message(conv_id, system_msg)
            
            # å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆå“åº”
            response = None
            if structured_content:
                response = await self.llm.generate_response(messages, structured_content)
                
                # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
                user_msg = Message(role="user", content=structured_content)
                messages.append(user_msg)
                self.conversation_manager.save_message(conv_id, user_msg)
                
                # ä¿å­˜åŠ©æ‰‹å“åº”
                assistant_msg = Message(role="assistant", content=response)
                messages.append(assistant_msg)
                self.conversation_manager.save_message(conv_id, assistant_msg)
            
            # å¦‚æœæ˜¯æœ€ç»ˆå¯¹è¯åˆ™ä¿å­˜åˆ°æ–‡ä»¶
            if is_final:
                await self.conversation_manager.save_conversation_to_file(conv_id)
            
            return {
                "conversation_id": conv_id,
                "response": response,
                "message_count": len(messages),
                "input_preview": (structured_content.to_display_text() 
                                if structured_content else None)
            }


async def comprehensive_demo():
    """
    ç»¼åˆåŠŸèƒ½æ¼”ç¤ºã€‚
    
    å±•ç¤ºç»“æ„åŒ–å†…å®¹å®šä½ã€å¯¹è¯æµç¨‹å’Œå„ç§ä½¿ç”¨æ¨¡å¼ã€‚
    """
    print("ğŸš€ ç‹¬ç«‹å¯¹è¯ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    graph = StandaloneConversationGraph()
    
    # æ¼”ç¤º 1: å¤æ‚ç»“æ„åŒ–å†…å®¹
    print("\n1ï¸âƒ£ å¤æ‚ç»“æ„åŒ–å†…å®¹ï¼š")
    content1 = StructuredMessageContent.from_mixed_items(
        "æ•°æ®åˆ†ææŠ¥å‘Š",
        "æ‰§è¡Œæ‘˜è¦ï¼š",
        {'json': {"çŠ¶æ€": "å·²å®Œæˆ", "å¾—åˆ†": 95}},
        "è¯¦ç»†å¯è§†åŒ–ï¼š",
        {'image': "analysis_chart.png"},
        "å»ºè®®ï¼š",
        {'json': {"è¡ŒåŠ¨": ["ä¼˜åŒ–", "æ‰©å±•", "ç›‘æ§"]}}
    )
    
    result1 = await graph.chat(
        system_prompt="ä½ æ˜¯æ•°æ®åˆ†æä¸“å®¶ã€‚",
        structured_content=content1
    )
    
    print(f"è¾“å…¥: {result1['input_preview']}")
    print(f"å›å¤: {result1['response']}\n")
    
    # æ¼”ç¤º 2: æ··åˆå†…å®¹ç±»å‹
    print("2ï¸âƒ£ æ··åˆå†…å®¹ç±»å‹ï¼š")
    content2 = StructuredMessageContent.from_mixed_items(
        "é¡¹ç›®å¼€å§‹",
        {'image': "intro_image.jpg"},
        "ä¸­é—´è¯´æ˜",
        {'json': {"é¡¹ç›®æ•°æ®": True}},
        "é¡¹ç›®ç»“è®º"
    )
    
    result2 = await graph.chat(
        conversation_id=result1['conversation_id'],
        structured_content=content2
    )
    
    print(f"è¾“å…¥: {result2['input_preview']}")
    print(f"å›å¤: {result2['response']}\n")
    
    # æ¼”ç¤º 3: æœ€ç»ˆæ¶ˆæ¯å¹¶ä¿å­˜
    print("3ï¸âƒ£ æœ€ç»ˆæ¶ˆæ¯å¹¶ä¿å­˜ï¼š")
    content3 = StructuredMessageContent.from_mixed_items(
        "æ„Ÿè°¢æ‚¨çš„åˆ†æï¼",
        "æ€»ç»“å›¾ç‰‡ï¼š",
        {'image': "summary_dashboard.png"}
    )
    
    result3 = await graph.chat(
        conversation_id=result1['conversation_id'],
        structured_content=content3,
        is_final=True
    )
    
    print(f"è¾“å…¥: {result3['input_preview']}")
    print(f"å›å¤: {result3['response']}")
    print(f"æ€»æ¶ˆæ¯æ•°: {result3['message_count']}")
    
    # æ¼”ç¤º 4: å¹¶å‘å¤„ç†æµ‹è¯•
    print("\n4ï¸âƒ£ å¹¶å‘å¤„ç†æµ‹è¯•ï¼š")
    tasks = []
    for i in range(3):
        content = StructuredMessageContent.from_mixed_items(
            f"ä»»åŠ¡ {i+1}",
            {'json': {"ä»»åŠ¡ID": i+1}}
        )
        
        task = graph.chat(structured_content=content)
        tasks.append(task)
    
    start_time = asyncio.get_event_loop().time()
    results = await asyncio.gather(*tasks)
    end_time = asyncio.get_event_loop().time()
    
    print(f"âœ… åœ¨ {end_time - start_time:.2f}s å†…å¤„ç†äº† {len(results)} ä¸ªå¯¹è¯")
    print("\nğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(comprehensive_demo())
