"""ç®€æ´ç¤ºä¾‹å’Œå®ç”¨å‡½æ•°ï¼Œæ¼”ç¤ºç»“æ„åŒ–å†…å®¹å®šä½ä¸ä½¿ç”¨æ¨¡å¼ã€‚"""

import asyncio
import os
import time
from dotenv import load_dotenv
load_dotenv()

# ç„¶åå¯¼å…¥conversationæ¨¡å—
from conversation.core import ConversationGraph, Content
from conversation.utils import get_logger
logger = get_logger(__name__)


class ConversationBuilder:
    """æ„å»ºå™¨ï¼šç”Ÿæˆæ¼”ç¤ºç”¨çš„å¤šè½®å¯¹è¯ä¸ç»“æ„åŒ–å†…å®¹ã€‚"""
    
    def __init__(self, llm_name: str = None):
        """åˆå§‹åŒ–å›¾å®ä¾‹ï¼Œé»˜è®¤ä½¿ç”¨mockæ¨¡å‹"""
        if llm_name is None:
            # ç®€å•åˆ¤æ–­ï¼Œä¼˜å…ˆä½¿ç”¨mocké¿å…ç½‘ç»œä¾èµ–
            llm_name = 'mock'
            print("ä½¿ç”¨ Mock æ¨¡å‹è¿›è¡Œæ¼”ç¤ºï¼ˆè½»é‡çº§æµ‹è¯•ï¼‰")
        
        self.graph = ConversationGraph(llm=llm_name)
    
    async def create_data_analysis_conversation(self) -> str:
        """ç¤ºä¾‹ï¼šåˆ›å»ºä¸€è½®æ•°æ®åˆ†æå¯¹è¯å¹¶è¿”å› conv_idã€‚"""
        print("ğŸ“Š åˆ›å»ºæ•°æ®åˆ†æå¯¹è¯...")
        
        # ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼šå¸¦ç»“æ„åŒ–æ•°æ®çš„ä»‹ç»
        intro_content = Content(
            "æˆ‘éœ€è¦åˆ†æä»¥ä¸‹ç”¨æˆ·æ•°æ®ã€‚",
            "é¦–å…ˆï¼Œè¿™æ˜¯ç”¨æˆ·åŸºæœ¬ä¿¡æ¯ï¼š",
            {'json': {
                "user_id": "U12345",
                "name": "å¼ æ•", 
                "age": 28,
                "location": "æ—§é‡‘å±±"
            }},
            "è¿™æ˜¯ä»–ä»¬çš„è¡Œä¸ºæˆªå›¾ï¼š",
            {'image': "test_image.jpg"},
            "è¯·æä¾›åˆæ­¥åˆ†æã€‚"
        )
        
        result1 = await self.graph.chat(
            system_prompt="ä½ æ˜¯èµ„æ·±ç”¨æˆ·è¡Œä¸ºåˆ†æå¸ˆï¼Œæ“…é•¿ä»å¤šæ¨¡æ€æ•°æ®ä¸­æå–æ´è§ã€‚",
            content=intro_content,
        )
        
        conv_id = result1['conv_id']
        print(f"åˆæ­¥åˆ†æ: {result1['response'][:100]}...")
        
        # åç»­è¯¦ç»†æŒ‡æ ‡
        metrics_content = Content(
            "æ ¹æ®ä½ çš„åˆ†æï¼Œä»¥ä¸‹æ˜¯è¯¦ç»†æŒ‡æ ‡ï¼š",
            {'json': {
                "sessions": [
                    {"date": "2025-01-01", "duration": 45, "pages": 12},
                    {"date": "2025-01-02", "duration": 32, "pages": 8}
                ],
                "conversion_rate": 0.15,
                "engagement_score": 8.5
            }},
            "ä»¥ä¸‹æ˜¯å¯è§†åŒ–å›¾è¡¨ï¼š",
            {'image': "test_image.jpg"},
            "ä½ çš„å»ºè®®æ˜¯ä»€ä¹ˆï¼Ÿ"
        )
        
        result2 = await self.graph.chat(
            conv_id=conv_id,
            content=metrics_content
        )
        
        print(f"æœ€ç»ˆå»ºè®®: {result2['response'][:100]}...")
        
        # ç»“æŸå¯¹è¯å¹¶ä¿å­˜
        await self.graph.end(conv_id, save=True)
        return conv_id
    
    async def create_product_presentation(self) -> str:
        """ç¤ºä¾‹ï¼šåˆ›å»ºäº§å“æ¼”ç¤ºå¯¹è¯å¹¶è¿”å› conv_idã€‚"""
        print("ğŸš€ åˆ›å»ºäº§å“æ¼”ç¤º...")
        presentation_content = Content(
            "æ–°å“å‘å¸ƒæ¼”ç¤º",
            {'image': "test_image.jpg"},
            "ä¸»è¦åŠŸèƒ½ï¼š",
            {'json': {
                "features": [
                    "åŸºäºAIçš„æ¨è",
                    "å®æ—¶åä½œ",
                    "å…ˆè¿›çš„åˆ†æèƒ½åŠ›"
                ],
                "target_audience": "ä¼ä¸šå®¢æˆ·"
            }},
            "æŠ€æœ¯è§„æ ¼ï¼š",
            {'json': {
                "performance": {
                    "response_time": "< 100ms",
                    "uptime": "99.9%",
                    "scalability": "1M+ ç”¨æˆ·"
                }
            }},
            "è¯·ç”Ÿæˆä¸€ä»½æœ‰å¸å¼•åŠ›çš„äº§å“æ‘˜è¦ã€‚"
        )

        result = await self.graph.chat(
            system_prompt="ä½ æ˜¯äº§å“è¥é”€ä¸“å®¶ï¼Œè´Ÿè´£æ’°å†™æœ‰å¸å¼•åŠ›çš„æ¼”ç¤ºæ–‡æ¡ˆã€‚",
            content=presentation_content
        )

        print(f"äº§å“æ‘˜è¦: {result['response'][:100]}...")
        
        # ç»“æŸå¯¹è¯å¹¶ä¿å­˜
        await self.graph.end(result['conv_id'], save=True)
        return result['conv_id']


async def demonstrate_custom_fields():
    """æ¼”ç¤ºè‡ªå®šä¹‰å­—æ®µåŠŸèƒ½ï¼šä¸ºå†…å®¹å—æ·»åŠ é¢å¤–å±æ€§ã€‚"""
    print("ğŸ¨ æ¼”ç¤ºè‡ªå®šä¹‰å­—æ®µåŠŸèƒ½...")
    
    # ä½¿ç”¨è½»é‡çº§mockæ¨¡å‹
    graph = ConversationGraph(llm='mock')
    
    # åˆ›å»ºå¸¦è‡ªå®šä¹‰å­—æ®µçš„å†…å®¹
    content = Content()
    content.add_text("é‡è¦é€šçŸ¥", style="bold", importance="high")
    content.add_image("test_image.jpg", alt_text="æµ‹è¯•å›¾ç‰‡", width=800)
    content.add_json({"sales": 150000, "growth": "12%"}, source="è´¢åŠ¡ç³»ç»Ÿ")
    
    print(f"å†…å®¹é¢„è§ˆ: {content.to_display_text()}")
    
    # æ˜¾ç¤ºè‡ªå®šä¹‰å­—æ®µ
    for i, block in enumerate(content.blocks):
        if block.extras:
            print(f"å—{i} è‡ªå®šä¹‰å­—æ®µ: {list(block.extras.keys())}")
    
    result = await graph.chat(content=content)
    print(f"å¯¹è¯ç»“æœ: {result['response'][:50]}...")
    
    await graph.end(result['conv_id'], save=False)  # ä¸ä¿å­˜ï¼Œç®€åŒ–æµç¨‹
    return result['conv_id']


async def demonstrate_positioning_control():
    """æ¼”ç¤ºå†…å®¹æ„é€ ï¼šé¡ºåºæ·»åŠ æ“ä½œç¤ºä¾‹ã€‚"""
    print("ğŸ¯ æ¼”ç¤ºå†…å®¹æ„é€ åŠŸèƒ½...")
    
    graph = ConversationGraph(llm='mock')
    
    # æµ‹è¯•1: é¡ºåºæ·»åŠ 
    content1 = Content()
    content1.add_text("å¼€å§‹").add_image("test_image.jpg").add_json({"test": 1})
    result1 = await graph.chat(content=content1)
    print(f"é¡ºåºæ·»åŠ : {result1['input_preview'][:50]}...")
    
    # æµ‹è¯•2: å·¥å‚æ–¹æ³•æ„é€ 
    content2 = Content(
        "æ ‡é¢˜", 
        {'image': 'test_image.jpg'}, 
        {'json': {'data': 123}}
    )
    result2 = await graph.chat(content=content2)
    print(f"å·¥å‚æ–¹æ³•: {result2['input_preview'][:50]}...")
    
    # æ¸…ç†
    await graph.end(result1['conv_id'], save=False)
    await graph.end(result2['conv_id'], save=False)


async def batch_conversation_processing():
    """æ‰¹é‡å¹¶å‘ç¤ºä¾‹ï¼Œè½»é‡çº§å¹¶å‘æµ‹è¯•ã€‚"""
    print("ğŸ”¥ æ‰¹é‡ä¼šè¯å¹¶å‘æµ‹è¯•...")
    
    graph = ConversationGraph(llm='mock', max_concurrent=5)
    
    # åˆ›å»ºç®€åŒ–çš„æµ‹è¯•ä»»åŠ¡
    tasks = []
    for i in range(3):  # å‡å°‘ä»»åŠ¡æ•°é‡ï¼Œæé«˜æ•ˆç‡
        content = Content(f"ä»»åŠ¡ #{i+1}: ç®€å•è®¡ç®—", {'json': {"id": i+1}})
        task = graph.chat(system_prompt="ç®€æ´å›ç­”", content=content)
        tasks.append(task)
    
    start_time = time.time()
    results = await asyncio.gather(*tasks)
    elapsed = time.time() - start_time
    
    print(f"âœ… å®Œæˆ {len(results)} ä¸ªä¼šè¯ï¼Œè€—æ—¶ {elapsed:.2f}s")
    
    # æ‰¹é‡æ¸…ç†
    cleanup_tasks = [graph.end(r['conv_id'], save=False) for r in results]
    await asyncio.gather(*cleanup_tasks)


async def concurrent_inference_test():
    """
    ç®€åŒ–çš„å¹¶å‘æ¨ç†æµ‹è¯•ï¼šæµ‹è¯•åŸºæœ¬çš„å¹¶å‘æ€§èƒ½
    """
    print("ğŸš€ å¹¶å‘æ¨ç†æµ‹è¯•...")
    
    # æµ‹è¯•ä¸åŒçš„å¹¶å‘çº§åˆ«
    concurrent_levels = [1, 3, 5]
    
    for concurrent in concurrent_levels:
        print(f"\nğŸ“Š æµ‹è¯•å¹¶å‘æ•°: {concurrent}")
        
        graph = ConversationGraph(llm='mock', max_concurrent=concurrent)
        
        # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
        prompts = ["è®¡ç®—1+1", "è§£é‡ŠAI", "å†™å‡½æ•°", "æ¨èä¹¦ç±"]
        tasks = []
        
        start_time = time.time()
        for i, prompt in enumerate(prompts[:concurrent]):
            content = Content(f"ä»»åŠ¡{i+1}: {prompt}")
            task = graph.chat(content=content)
            tasks.append(task)
        
        # æ‰§è¡Œå¹¶ç»Ÿè®¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        elapsed = time.time() - start_time
        
        successful = [r for r in results if not isinstance(r, Exception)]
        
        print(f"  å®Œæˆç‡: {len(successful)}/{len(results)}")
        print(f"  è€—æ—¶: {elapsed:.2f}s")
        print(f"  åå: {len(successful)/elapsed:.1f} req/s")
        
        # æ¸…ç†
        cleanup_tasks = [
            graph.end(r['conv_id'], save=False) 
            for r in successful if hasattr(r, 'get') and r.get('conv_id')
        ]
        if cleanup_tasks:
            await asyncio.gather(*cleanup_tasks, return_exceptions=True)


async def multi_round_conversation_test():
    """å¤šè½®å¯¹è¯æµ‹è¯•"""
    print("ï¿½ å¤šè½®å¯¹è¯æµ‹è¯•...")
    
    graph = ConversationGraph(llm='mock')
    
    # ç¬¬ä¸€è½®
    content1 = Content("ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰")
    result1 = await graph.chat(system_prompt="è®°ä½ç”¨æˆ·ä¿¡æ¯", content=content1)
    conv_id = result1['conv_id']
    
    # ç¬¬äºŒè½® - æµ‹è¯•è®°å¿†
    content2 = Content("æˆ‘åˆšæ‰è¯´æˆ‘å«ä»€ä¹ˆï¼Ÿ")
    result2 = await graph.chat(conv_id=conv_id, content=content2)
    
    # ç¬¬ä¸‰è½® - å¸¦ç»“æ„åŒ–æ•°æ®
    content3 = Content(
        "æœ€åä¸€ä¸ªé—®é¢˜ï¼š",
        {'json': {"question": "æ€»ç»“å¯¹è¯", "round": 3}}
    )
    result3 = await graph.chat(conv_id=conv_id, content=content3)
    
    print(f"âœ… å®Œæˆ3è½®å¯¹è¯ï¼Œå…±{result3['message_count']}æ¡æ¶ˆæ¯")
    
    await graph.end(conv_id, save=True)  # ä¿å­˜å®Œæ•´å¯¹è¯
    return conv_id


async def main():
    """è¿è¡Œæ‰€æœ‰æ¼”ç¤ºåœºæ™¯çš„ä¸»å‡½æ•°ã€‚"""
    print("ğŸ¤– å¯¹è¯ç³»ç»Ÿæ¼”ç¤º (è½»é‡ç‰ˆ)")
    print("=" * 40)
    
    builder = ConversationBuilder()
    
    # æ ¸å¿ƒåŠŸèƒ½æ¼”ç¤º
    print("\n1ï¸âƒ£ æ•°æ®åˆ†æåœºæ™¯:")
    await builder.create_data_analysis_conversation()
    
    print("\n2ï¸âƒ£ äº§å“æ¼”ç¤ºåœºæ™¯:")
    await builder.create_product_presentation()
    
    print("\n3ï¸âƒ£ è‡ªå®šä¹‰å­—æ®µæ¼”ç¤º:")
    await demonstrate_custom_fields()
    
    print("\n4ï¸âƒ£ å†…å®¹æ„é€ æ¼”ç¤º:")
    await demonstrate_positioning_control()
    
    print("\n5ï¸âƒ£ æ‰¹é‡å¤„ç†æ¼”ç¤º:")
    await batch_conversation_processing()
    
    print("\n6ï¸âƒ£ å¹¶å‘æµ‹è¯•:")
    await concurrent_inference_test()
    
    print("\n7ï¸âƒ£ å¤šè½®å¯¹è¯æµ‹è¯•:")
    await multi_round_conversation_test()
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())
