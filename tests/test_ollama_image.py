#!/usr/bin/env python3
"""
æµ‹è¯•Ollamaå›¾ç‰‡å¤„ç†åŠŸèƒ½
"""

import asyncio
import os
from conversation.core import Content, ConversationGraph

async def test_ollama_image():
    """æµ‹è¯•Ollamaå¤„ç†å›¾ç‰‡çš„èƒ½åŠ›"""
    print("ğŸ–¼ï¸ æµ‹è¯•Ollamaå›¾ç‰‡å¤„ç†åŠŸèƒ½...")
    
    # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    image_paths = [
        "./data/images/test_image.jpg",
        "./images/test_image.jpg", 
        "./test_image.jpg"
    ]
    
    existing_image = None
    for path in image_paths:
        if os.path.exists(path):
            existing_image = path
            break
    
    if not existing_image:
        print("âš ï¸ æœªæ‰¾åˆ°æµ‹è¯•å›¾ç‰‡ï¼Œåˆ›å»ºæ–‡æœ¬æµ‹è¯•...")
        content = Content()
        content.add_text("è¯·ä»‹ç»ä¸€ä¸‹ä½ çš„å¤šæ¨¡æ€å¤„ç†èƒ½åŠ›")
    else:
        print(f"âœ… æ‰¾åˆ°æµ‹è¯•å›¾ç‰‡: {existing_image}")
        content = Content()
        content.add_text("è¯·æè¿°è¿™å¼ å›¾ç‰‡ä¸­çš„å†…å®¹")
        content.add_image(existing_image)
    
    print(f"ğŸ“ å‘é€å†…å®¹: {content.to_display_text()}")
    
    # åˆ›å»ºå¯¹è¯å›¾ï¼Œå¼ºåˆ¶ä½¿ç”¨ollama
    graph = ConversationGraph(llm='ollama')
    
    try:
        # å‘é€è¯·æ±‚
        result = await graph.chat(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾ç‰‡åˆ†æåŠ©æ‰‹ï¼Œèƒ½å¤Ÿè¯¦ç»†æè¿°å›¾ç‰‡å†…å®¹ã€‚",
            content=content
        )
        
        print("\nğŸ¤– Ollamaå›å¤:")
        print("=" * 50)
        print(result['response'])
        print("=" * 50)
        print(f"ğŸ“Š å¯¹è¯ç»Ÿè®¡: {result['message_count']} æ¡æ¶ˆæ¯")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥:")
        print("1. OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ: ollama serve")
        print("2. æ˜¯å¦å®‰è£…äº†æ”¯æŒè§†è§‰çš„æ¨¡å‹: ollama pull qwen2.5vl:3b") 
        print("3. .envä¸­OLLAMA_MODELé…ç½®æ˜¯å¦æ­£ç¡®")

if __name__ == "__main__":
    asyncio.run(test_ollama_image())
